{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udit/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bmv_training_set.csv')\n",
    "test_data = pd.read_csv('./bmv_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_attributes(s):\n",
    "    a = dict(id=s['id'], additionalAttributes=s['additionalAttributes'], label=s['label'])\n",
    "    for i in s['additionalAttributes'].split(\";\"):\n",
    "        x = i.split(\"=\")\n",
    "        if len(x) == 2:\n",
    "            a[x[0]] = x[1]\n",
    "        else:\n",
    "            y = i.split(\":\")\n",
    "            if len(y) == 2:\n",
    "                a[y[0]] = y[1]\n",
    "            else:\n",
    "                print \"====\",i\n",
    "\n",
    "    return pd.Series(dict(id=s['id'], additionalAttributes=s['additionalAttributes'], label=s['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_transformation(data,test_data,test_size=0.3):\n",
    "    if type(test_data) == type(None):\n",
    "        train_data ,test_data = train_test_split(data,test_size=test_size)\n",
    "        return train_data['additionalAttributes'], train_data['label'], test_data['additionalAttributes'], test_data['label']\n",
    "    else:\n",
    "        return data['additionalAttributes'], data['label'], test_data['additionalAttributes'], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MultinomialNB_classifier(x_train,y_train,x_test,y_test,apply_grid_search=False,is_training=True):\n",
    "    text_clf = Pipeline([('vect', StemmedCountVectorizer(ngram_range=(1,2),stop_words='english')),\n",
    "                   ('tfidf', TfidfTransformer(use_idf=True,norm='l1')),\n",
    "                   ('clf', MultinomialNB(fit_prior=False)),])\n",
    "    if apply_grid_search:\n",
    "        parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                       'tfidf__use_idf': (True, False),\n",
    "                       'tfidf__norm' :('l1','l2',None),\n",
    "                       'clf__fit_prior': (True, False),\n",
    "         }\n",
    "        gs_clf_mnb = GridSearchCV(text_clf, parameters_svm, n_jobs=-1)\n",
    "        gs_clf_mnb = gs_clf_mnb.fit(x_train, y_train)\n",
    "        print gs_clf_mnb.best_score_\n",
    "        print gs_clf_mnb.best_params_\n",
    "        predicted = text_clf.predict(x_test)\n",
    "    else:\n",
    "        text_clf = text_clf.fit(x_train, y_train)\n",
    "        predicted = text_clf.predict(x_test)\n",
    "    if is_training and type(y_test) != type(None):\n",
    "        return np.mean(predicted == y_test)\n",
    "    else:\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_classifier(x_train,y_train,x_test,y_test,apply_grid_search=False,is_training=True):\n",
    "    text_clf_svm = Pipeline([('vect', StemmedCountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
    "                          ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                          ('clf-svm', SVC(kernel='linear', C = 10.0,degree=10,probability=True,tol=1e-3)),\n",
    "     ])\n",
    "    if apply_grid_search:\n",
    "        parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                       'tfidf__use_idf': (True, False),\n",
    "                       'tfidf__norm' :('l1','l2',None),\n",
    "                       'clf-svm__tol': (1e-2, 1e-3),\n",
    "                        'clf-svm__kernel' :('linear', 'poly', 'rbf', 'sigmoid')\n",
    "         }\n",
    "        gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "        gs_clf_svm = gs_clf_svm.fit(x_train, y_train)\n",
    "        print gs_clf_svm.best_score_\n",
    "        print gs_clf_svm.best_params_\n",
    "        predicted_svm = gs_clf_svm.predict(x_test)\n",
    "    else:\n",
    "        _ = text_clf_svm.fit(x_train, y_train)\n",
    "        predicted_svm = text_clf_svm.predict(x_test)\n",
    "    if is_training and type(y_test) != type(None):\n",
    "        return np.mean(predicted_svm == y_test)\n",
    "    else:\n",
    "        return predicted_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boosting_classifier(x_train,y_train,x_test,y_test,apply_grid_search=False,is_training=True):\n",
    "    text_clf_boosting = Pipeline([('vect', StemmedCountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
    "                          ('tfidf', TfidfTransformer(use_idf=True,norm='l1')),\n",
    "                          ('clf-boosting', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=99,\n",
    "                                                                                     min_samples_split=3,\n",
    "                                                                                     criterion=\"gini\",\n",
    "                                                                                     splitter=\"best\",\n",
    "                                                                                     max_depth=11),\n",
    "                                                              n_estimators=100,learning_rate=.1)),\n",
    "     ])\n",
    "    if apply_grid_search:\n",
    "        parameters_boosting = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                       'tfidf__use_idf': (True, False),\n",
    "                       'tfidf__norm' :('l1','l2',None),\n",
    "                       'clf-boosting__learning_rate': (.1, .2,.3),\n",
    "         }\n",
    "        gs_clf_boosting = GridSearchCV(text_clf_boosting, parameters_boosting, n_jobs=-1)\n",
    "        gs_clf_boosting = gs_clf_boosting.fit(x_train,y_train)\n",
    "        print gs_clf_boosting.best_score_\n",
    "        print gs_clf_boosting.best_params_\n",
    "        predicted_svm = gs_clf_boosting.predict(x_test)\n",
    "    else:\n",
    "        _ = text_clf_boosting.fit(x_train,y_train)\n",
    "        predicted_boosting = text_clf_boosting.predict(x_test)\n",
    "    if is_training and type(y_test) != type(None):\n",
    "        return np.mean(predicted_boosting == y_test)\n",
    "    else:\n",
    "        return predicted_boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_classifier(x_train,y_train,x_test,y_test,apply_grid_search=False,is_training=True):\n",
    "    text_clf_sgd = Pipeline([('vect', StemmedCountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
    "                          ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                          ('clf-sgd', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                                alpha=1e-3, n_iter=5, random_state=42)),\n",
    "     ])\n",
    "    if apply_grid_search:\n",
    "        parameters_sgd = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                       'tfidf__use_idf': (True, False),\n",
    "                       'tfidf__norm' :('l1','l2',None),\n",
    "                       'clf-sgd__alpha': (1e-2, 1e-3),\n",
    "         }\n",
    "        gs_clf_sgd = GridSearchCV(text_clf_sgd, parameters_sgd, n_jobs=-1)\n",
    "        gs_clf_sgd = gs_clf_sgd.fit(x_train,y_train)\n",
    "        print gs_clf_sgd.best_score_\n",
    "        print gs_clf_sgd.best_params_\n",
    "        predicted_sgd = gs_clf_sgd.predict(x_test)\n",
    "    else:\n",
    "        _ = text_clf_sgd.fit(x_train,y_train)\n",
    "        predicted_sgd = text_clf_sgd.predict(x_test)\n",
    "    if is_training and type(y_test) != type(None):\n",
    "        return np.mean(predicted_sgd == y_test)\n",
    "    else:\n",
    "        return predicted_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = tfidf_transformation(data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print boosting_classifier(x_train,y_train,x_test,y_test,False)\n",
    "multinomialnb_results = MultinomialNB_classifier(x_train,y_train,x_test,None,False,False)\n",
    "# print svm_classifier(x_train,y_train,x_test,y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udit/anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "sgd_results = sgd_classifier(x_train,y_train,x_test,None,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_whole_test_data= test_data\n",
    "sgd_whole_test_data['label'] = sgd_results\n",
    "sgd_whole_test_data.to_csv('sgd_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multinomialnb_whole_test_data= test_data\n",
    "multinomialnb_whole_test_data['label'] = multinomialnb_results\n",
    "multinomialnb_whole_test_data.to_csv('multinomialnb_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For using grid search to find the parameters and getting the accuracy on the train model\n",
    "# Please note it will take a lot of time on laptops (Use servers Algorithms already optimized for multiple cores)\n",
    "# x_train,y_train,x_test,y_test = tfidf_transformation(data,None)\n",
    "# print MultinomialNB_classifier(x_train,y_train,x_test,y_test,True,True)\n",
    "# print sgd_classifier(x_train,y_train,x_test,y_test,True,True)\n",
    "\n",
    "# For just finding out the accuracy (without using grid search). It will follow the hard corded parameters only\n",
    "# print MultinomialNB_classifier(x_train,y_train,x_test,y_test,False,True)\n",
    "# print sgd_classifier(x_train,y_train,x_test,y_test,False,True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
